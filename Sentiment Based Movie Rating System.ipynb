{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as num\n",
    "import pandas as pd \n",
    "import nltk\n",
    "import inflect\n",
    "import re,unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords \n",
    "#nltk.download('punkt')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import naive_bayes\n",
    "import time\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"kdnuggetstrain.tsv\",na_filter=False,skip_blank_lines=True,header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)        \n",
    "train.columns.values\n",
    "x=num.array([id,\"sentiment\",\"review\"], \n",
    "           dtype=object) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cList = {\n",
    "  \"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"can not\",\"can't've\": \"can not have\",\"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"he'll\": \"he will\",  \"he's\": \"he is\",\n",
    "  \"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\"he'll've\": \"he will have\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",  \"how's\": \"how is\",  \"I'd\": \"I would\",  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",  \"I'll've\": \"I will have\",  \"I'm\": \"I am\",  \"I've\": \"I have\",\"i've\": \"i have\",\n",
    "  \"isn't\": \"is not\",  \"it'd\": \"it had\",  \"it'd've\": \"it would have\",  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",  \"it's\": \"it is\",  \"let's\": \"let us\",  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",  \"might've\": \"might have\",  \"mightn't\": \"might not\",  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",  \"mustn't\": \"must not\",  \"mustn't've\": \"must not have\",  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",  \"o'clock\": \"of the clock\",  \"oughtn't\": \"ought not\",  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",  \"sha'n't\": \"shall not\",  \"shan't've\": \"shall not have\",  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",  \"she'll\": \"she will\",  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",  \"should've\": \"should have\", \"what'll\": \"what will\",  \"what'll've\": \"what will have\",\n",
    "  \"shouldn't\": \"should not\",  \"shouldn't've\": \"should not have\",  \"so've\": \"so have\",  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",  \"that'd've\": \"that would have\",  \"that's\": \"that is\",  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",  \"there's\": \"there is\",  \"they'd\": \"they would\",  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",  \"they'll've\": \"they will have\",  \"they're\": \"they are\",  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",  \"wasn't\": \"was not\",  \"we'd\": \"we had\",  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",  \"we'll've\": \"we will have\",  \"we're\": \"we are\",  \"we've\": \"we have\",  \"weren't\": \"were not\",\n",
    "   \"what're\": \"what are\",  \"what's\": \"what is\",  \"what've\": \"what have\",  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",  \"where'd\": \"where did\",  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",  \"who'll\": \"who will\",  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",  \"who've\": \"who have\",  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",  \"will've\": \"will have\",  \"won't\": \"will not\",  \"won't've\": \"will not have\",  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",  \"wouldn't've\": \"would not have\",  \"y'all\": \"you all\",  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",  \"y'all'd've\": \"you all would have\",  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",  \"you'd\": \"you had\",  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you you will\",  \"you'll've\": \"you you will have\",  \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "b=strip_html(train[\"review\"][9])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(\"^a-zA-Z\", \" \", text)\n",
    "b=remove_between_square_brackets(train[\"review\"][9])\n",
    "print(b)\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expandContractions(words, c_re=c_re):\n",
    "    def replace(match):\n",
    "       return cList[match.group(0)]\n",
    "    return c_re.sub(replace,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = expandContractions(text)\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n",
    "b=denoise_text(train[\"review\"][9])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_numbers(words):\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words):\n",
    "    \n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "def lemmatize_verbs(words):\n",
    "  \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v'or'n')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(words):\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_preprocessing(words):\n",
    "    words = denoise_text(words)\n",
    "    words=normalize(words)\n",
    "    words=lemmatize(words)\n",
    "    a=[]\n",
    "    for word in words:\n",
    "        if ((len(word))>2):\n",
    "            a.append(word)\n",
    "    return(\" \".join(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[\"review\"][9])\n",
    "b=final_preprocessing(train[\"review\"][9])\n",
    "print(b)\n",
    "print(train.shape)\n",
    "print (\"Cleaning and parsing the training set movie reviews...\\n\")\n",
    "clean_train_reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,3033) :    \n",
    "    clean_review = final_preprocessing( train[\"review\"][i] )\n",
    "    clean_train_reviews.append(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "                   \n",
    "vectorizer = TfidfVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,\\\n",
    "                             lowercase=True, \\\n",
    "                             max_features = 10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3033, 10000)\n"
     ]
    }
   ],
   "source": [
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "train_data_features = train_data_features.toarray()\n",
    "print (train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(606, 2)\n",
      "CLEANING AND PARSING THE TEST SET MOVIE REVIEWS...\n",
      "\n",
      "PRESS 1 FOR NAIVE BAYES 2 FOR RANDOM FOREST...\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "#print (vocab)\n",
    "#Sum up the counts of each vocabulary word\n",
    "dist = num.sum(train_data_features, axis=0)\n",
    "##vocabulary word and the number of times it \n",
    "#for tag, count in zip(vocab, dist):\n",
    "#    print(count, tag)\n",
    "#\n",
    "test = pd.read_csv(\"kdnuggetstest.tsv\",na_filter=False,skip_blank_lines=True,header=0, delimiter=\"\\t\", \\\n",
    "quoting=3) \n",
    "print (test.shape)\n",
    "clean_test_reviews = [] \n",
    "\n",
    "num_reviews=len(test[\"review\"])\n",
    "print (\"CLEANING AND PARSING THE TEST SET MOVIE REVIEWS...\\n\")\n",
    "for i in range(0,606):\n",
    "        clean_review =final_preprocessing( test[\"review\"][i] )\n",
    "        clean_test_reviews.append(clean_review)\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()\n",
    "#training_model=int(input(\"PRESS 1 FOR NAIVE BAYES 2 FOR RANDOM FOREST...\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Naive Bayes\n",
      "WROTE RESULTS finalnaive_bayes.CSV\n",
      "accuracy of naive_bayes=\n",
      "83.33333333333334\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes \n",
    "bayes = naive_bayes.MultinomialNB()\n",
    "print (\"Training Naive Bayes\")\n",
    "\n",
    "bayess = bayes.fit(train_data_features, train[\"sentiment\"])\n",
    "result = bayess.predict(test_data_features)\n",
    "\n",
    "bayes = naive_bayes.MultinomialNB()\n",
    "rating = bayes.fit(train_data_features,train[\"id\"])\n",
    "\n",
    "result1 = rating.predict(test_data_features)\n",
    "result1 = result1.tolist()\n",
    "c = []\n",
    "for a in result1:\n",
    "    b=(a.split(\"_\")[1])\n",
    "    m = re.findall(r'\\d+',b)\n",
    "    c.append(m[0])\n",
    "result1n = num.array(c)    \n",
    "\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"],\"rating\":result1n,\"sentiment\":result} )\n",
    "output.to_csv( \"finalnaivebayes.csv\", index=False, quoting=3,escapechar='\\\\') \n",
    "print(\"WROTE RESULTS finalnaive_bayes.CSV\")\n",
    "model = pd.read_csv(\"finalnaivebayes.csv\")\n",
    "count=0\n",
    "for i in range(0,606):\n",
    "    if (((int(model[\"id\"][i].split(\"_\")[1]) <= 5) and (model[\"sentiment\"][i] == 0)) or\n",
    "          ((model[\"sentiment\"][i] == 1) and (int(model[\"id\"][i].split(\"_\")[1])) > 5)):\n",
    "        count=count+1\n",
    "a=(count/606)*100\n",
    "print(\"accuracy of naive_bayes=\")\n",
    "print(a)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting test labels\n",
      "WROTE RESULTS finalrandom_forest.CSV\n",
      "accuracy of random forest=\n",
      "81.68316831683168\n",
      "1088.7025737762451 seconds\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "forest = RandomForestClassifier(n_estimators = 100,n_jobs=-1,max_features=100)\n",
    "forests = forest.fit( train_data_features,train[\"sentiment\"])\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()\n",
    "print(\"predicting test labels\")\n",
    "result = forests.predict(test_data_features)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100,n_jobs=-1,max_features=100)\n",
    "rating = forest.fit(train_data_features,train[\"id\"])\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "result1 = rating.predict(test_data_features)\n",
    "result1 = result1.tolist()\n",
    "c = []\n",
    "for a in result1:\n",
    "    b=(a.split(\"_\")[1])\n",
    "    m = re.findall(r'\\d+',b)\n",
    "    c.append(m[0])\n",
    "result1n = num.array(c)    \n",
    "\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"],\"rating\":result1n,\"sentiment\":result} )\n",
    "output.to_csv( \"finalrandom_forest.csv\", index=False, quoting=3,escapechar='\\\\')\n",
    "print(\"WROTE RESULTS finalrandom_forest.CSV\")\n",
    "model = pd.read_csv(\"finalrandom_forest.csv\")\n",
    "count=0\n",
    "for i in range(0,606):\n",
    "    if (((int(model[\"id\"][i].split(\"_\")[1]) <= 5) and (model[\"sentiment\"][i] == 0)) or\n",
    "          ((model[\"sentiment\"][i] == 1) and (int(model[\"id\"][i].split(\"_\")[1])) > 5)):\n",
    "        count=count+1\n",
    "a=(count/606)*100\n",
    "print(\"accuracy of random forest=\")\n",
    "print(a)\n",
    "\n",
    "print(\"%s seconds\" %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
